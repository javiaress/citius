
=== Fold 0 ===
torch.Size([16])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')



torch.Size([])
tensor(2, device='cuda:0')



Epoch 0: | Train Loss: 9.522353 | Val Loss: 0.864144 | Train Acc: 71.736 | Val Acc: 82.486
Epoch 1: | Train Loss: 0.664153 | Val Loss: 0.764804 | Train Acc: 82.523 | Val Acc: 82.623
Epoch 2: | Train Loss: 0.635916 | Val Loss: 1.985994 | Train Acc: 82.523 | Val Acc: 81.318
Epoch 3: | Train Loss: 0.611185 | Val Loss: 0.770414 | Train Acc: 83.270 | Val Acc: 79.507
Epoch 4: | Train Loss: 0.565748 | Val Loss: 0.804865 | Train Acc: 83.438 | Val Acc: 82.477
Epoch 5: | Train Loss: 0.585650 | Val Loss: 0.752407 | Train Acc: 82.984 | Val Acc: 82.496
Epoch 6: | Train Loss: 0.846234 | Val Loss: 2.182853 | Train Acc: 83.446 | Val Acc: 81.785
Epoch 7: | Train Loss: 0.604456 | Val Loss: 0.740348 | Train Acc: 83.372 | Val Acc: 82.642
Epoch 8: | Train Loss: 0.552618 | Val Loss: 0.847098 | Train Acc: 83.526 | Val Acc: 82.613
Epoch 9: | Train Loss: 0.521856 | Val Loss: 0.831732 | Train Acc: 83.607 | Val Acc: 82.623
Epoch 10: | Train Loss: 1.047115 | Val Loss: 0.799034 | Train Acc: 83.394 | Val Acc: 82.662
Epoch 11: | Train Loss: 0.514355 | Val Loss: 0.818451 | Train Acc: 83.577 | Val Acc: 82.516
Epoch 12: | Train Loss: 0.588629 | Val Loss: 0.840569 | Train Acc: 83.446 | Val Acc: 82.642
Epoch 13: | Train Loss: 1.269940 | Val Loss: 1.410552 | Train Acc: 83.555 | Val Acc: 81.805
Epoch 14: | Train Loss: 1.086646 | Val Loss: 0.815954 | Train Acc: 83.343 | Val Acc: 82.603
Epoch 15: | Train Loss: 0.623672 | Val Loss: 0.849727 | Train Acc: 83.409 | Val Acc: 82.525
Epoch 16: | Train Loss: 0.508325 | Val Loss: 0.845705 | Train Acc: 83.511 | Val Acc: 82.623
Epoch 17: | Train Loss: 0.503896 | Val Loss: 0.813536 | Train Acc: 83.570 | Val Acc: 82.632
Epoch 18: | Train Loss: 0.710324 | Val Loss: 0.826148 | Train Acc: 83.182 | Val Acc: 82.632
Epoch 19: | Train Loss: 1.355926 | Val Loss: 0.837315 | Train Acc: 83.519 | Val Acc: 82.632
Epoch 20: | Train Loss: 0.514114 | Val Loss: 0.809689 | Train Acc: 83.636 | Val Acc: 82.642
Epoch 21: | Train Loss: 0.500159 | Val Loss: 0.855616 | Train Acc: 83.482 | Val Acc: 82.496
Epoch 22: | Train Loss: 0.502619 | Val Loss: 0.829087 | Train Acc: 83.394 | Val Acc: 82.632
Epoch 23: | Train Loss: 0.520360 | Val Loss: 0.886094 | Train Acc: 83.563 | Val Acc: 82.632
Epoch 24: | Train Loss: 0.506295 | Val Loss: 0.816670 | Train Acc: 83.497 | Val Acc: 82.642
Epoch 25: | Train Loss: 0.533329 | Val Loss: 0.842016 | Train Acc: 83.519 | Val Acc: 82.613
Epoch 26: | Train Loss: 0.530265 | Val Loss: 0.855967 | Train Acc: 83.504 | Val Acc: 82.603
Epoch 27: | Train Loss: 12.847152 | Val Loss: 0.868926 | Train Acc: 83.058 | Val Acc: 82.613
Epoch 28: | Train Loss: 0.514671 | Val Loss: 0.816862 | Train Acc: 83.555 | Val Acc: 82.642
Epoch 29: | Train Loss: 0.494922 | Val Loss: 0.823904 | Train Acc: 83.577 | Val Acc: 82.613
Epoch 30: | Train Loss: 0.496807 | Val Loss: 0.856095 | Train Acc: 83.614 | Val Acc: 82.623
Epoch 31: | Train Loss: 0.780598 | Val Loss: 0.830293 | Train Acc: 83.570 | Val Acc: 82.593
Epoch 32: | Train Loss: 0.497513 | Val Loss: 0.844197 | Train Acc: 83.563 | Val Acc: 82.642
Epoch 33: | Train Loss: 0.494728 | Val Loss: 0.836012 | Train Acc: 83.863 | Val Acc: 82.603
Epoch 34: | Train Loss: 0.494312 | Val Loss: 0.827629 | Train Acc: 83.811 | Val Acc: 82.632
Epoch 35: | Train Loss: 0.493941 | Val Loss: 0.838986 | Train Acc: 83.731 | Val Acc: 82.623
Epoch 36: | Train Loss: 0.496972 | Val Loss: 0.852362 | Train Acc: 83.621 | Val Acc: 82.574
Epoch 37: | Train Loss: 0.493563 | Val Loss: 0.842886 | Train Acc: 83.877 | Val Acc: 82.632
Early stopping


 real:
tensor([ 3,  2,  5,  6,  5, 14,  1, 14,  4,  3,  4,  4,  3,  2,  2, 14],
       device='cuda:0')


 pred:
tensor([ 3,  2,  3,  3,  3, 14,  2, 14,  4,  3,  4,  4,  3,  2,  2, 14],
       device='cuda:0')





 real:
tensor([ 2,  1,  2,  4,  4,  3,  4,  4,  5,  3,  3,  2,  2, 14,  2,  2],
       device='cuda:0')


 pred:
tensor([ 2,  2,  2,  4,  4,  4,  4,  4,  3,  3,  1,  3,  2, 14,  2,  2],
       device='cuda:0')





 real:
tensor([ 4, 14,  1, 14,  2,  2, 14,  8,  2,  3,  4, 14,  2,  3,  3,  3],
       device='cuda:0')


 pred:
tensor([ 4, 14,  2, 14,  2,  2, 14,  3,  2,  4,  4, 14,  2,  3,  3,  3],
       device='cuda:0')



Fold 0 Levenshtein Accuracy: 0.8286

=== Fold 1 ===
torch.Size([16])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2], device='cuda:0')



torch.Size([])
tensor(5, device='cuda:0')



[NaN/Inf DETECTED] in dA at step 12
dA.min: -inf, max: inf
tensor([[[ 9.9860e-01, -2.8599e-03, -4.2897e-03,  ..., -4.2897e-02,
          -4.4328e-02, -4.5756e-02],
         [ 1.0822e-03,  1.0022e+00,  3.2466e-03,  ...,  3.2465e-02,
           3.3547e-02,  3.4630e-02],
         [ 7.1959e-03,  1.4392e-02,  1.0216e+00,  ...,  2.1587e-01,
           2.2307e-01,  2.3025e-01],
         ...,
         [-3.4198e-03, -6.8396e-03, -1.0259e-02,  ...,  8.9741e-01,
          -1.0601e-01, -1.0943e-01],
         [-9.3533e-03, -1.8706e-02, -2.8059e-02,  ..., -2.8060e-01,
           7.1006e-01, -2.9929e-01],
         [ 4.1886e-03,  8.3769e-03,  1.2565e-02,  ...,  1.2565e-01,
           1.2984e-01,  1.1340e+00]],

        [[ 9.9860e-01, -2.8599e-03, -4.2897e-03,  ..., -4.2897e-02,
          -4.4328e-02, -4.5756e-02],
         [ 1.0822e-03,  1.0022e+00,  3.2466e-03,  ...,  3.2465e-02,
           3.3547e-02,  3.4630e-02],
         [ 7.1959e-03,  1.4392e-02,  1.0216e+00,  ...,  2.1587e-01,
           2.2307e-01,  2.3025e-01],
         ...,
         [-3.4198e-03, -6.8396e-03, -1.0259e-02,  ...,  8.9741e-01,
          -1.0601e-01, -1.0943e-01],
         [-9.3533e-03, -1.8706e-02, -2.8059e-02,  ..., -2.8060e-01,
           7.1006e-01, -2.9929e-01],
         [ 4.1886e-03,  8.3769e-03,  1.2565e-02,  ...,  1.2565e-01,
           1.2984e-01,  1.1340e+00]],

        [[ 9.9860e-01, -2.8599e-03, -4.2897e-03,  ..., -4.2897e-02,
          -4.4328e-02, -4.5756e-02],
         [ 1.0822e-03,  1.0022e+00,  3.2466e-03,  ...,  3.2465e-02,
           3.3547e-02,  3.4630e-02],
         [ 7.1959e-03,  1.4392e-02,  1.0216e+00,  ...,  2.1587e-01,
           2.2307e-01,  2.3025e-01],
         ...,
         [-3.4198e-03, -6.8396e-03, -1.0259e-02,  ...,  8.9741e-01,
          -1.0601e-01, -1.0943e-01],
         [-9.3533e-03, -1.8706e-02, -2.8059e-02,  ..., -2.8060e-01,
           7.1006e-01, -2.9929e-01],
         [ 4.1886e-03,  8.3769e-03,  1.2565e-02,  ...,  1.2565e-01,
           1.2984e-01,  1.1340e+00]],

        ...,

        [[ 9.9902e-01, -2.0219e-03, -3.0323e-03,  ..., -3.0328e-02,
          -3.1336e-02, -3.2349e-02],
         [ 4.9003e-03,  1.0099e+00,  1.4699e-02,  ...,  1.4701e-01,
           1.5190e-01,  1.5681e-01],
         [ 1.2582e-03,  2.5166e-03,  1.0037e+00,  ...,  3.7747e-02,
           3.9001e-02,  4.0262e-02],
         ...,
         [-7.4138e-03, -1.4828e-02, -2.2239e-02,  ...,  7.7760e-01,
          -2.2981e-01, -2.3723e-01],
         [-1.1448e-02, -2.2897e-02, -3.4340e-02,  ..., -3.4344e-01,
           6.4508e-01, -3.6633e-01],
         [ 7.9046e-03,  1.5810e-02,  2.3711e-02,  ...,  2.3714e-01,
           2.4502e-01,  1.2529e+00]],

        [[ 9.9902e-01, -2.0219e-03, -3.0323e-03,  ..., -3.0328e-02,
          -3.1336e-02, -3.2349e-02],
         [ 4.9003e-03,  1.0099e+00,  1.4699e-02,  ...,  1.4701e-01,
           1.5190e-01,  1.5681e-01],
         [ 1.2582e-03,  2.5166e-03,  1.0037e+00,  ...,  3.7747e-02,
           3.9001e-02,  4.0262e-02],
         ...,
         [-7.4138e-03, -1.4828e-02, -2.2239e-02,  ...,  7.7760e-01,
          -2.2981e-01, -2.3723e-01],
         [-1.1448e-02, -2.2897e-02, -3.4340e-02,  ..., -3.4344e-01,
           6.4508e-01, -3.6633e-01],
         [ 7.9046e-03,  1.5810e-02,  2.3711e-02,  ...,  2.3714e-01,
           2.4502e-01,  1.2529e+00]],

        [[ 9.9860e-01, -2.8599e-03, -4.2897e-03,  ..., -4.2897e-02,
          -4.4328e-02, -4.5756e-02],
         [ 1.0822e-03,  1.0022e+00,  3.2466e-03,  ...,  3.2465e-02,
           3.3547e-02,  3.4630e-02],
         [ 7.1959e-03,  1.4392e-02,  1.0216e+00,  ...,  2.1587e-01,
           2.2307e-01,  2.3025e-01],
         ...,
         [-3.4198e-03, -6.8396e-03, -1.0259e-02,  ...,  8.9741e-01,
          -1.0601e-01, -1.0943e-01],
         [-9.3533e-03, -1.8706e-02, -2.8059e-02,  ..., -2.8060e-01,
           7.1006e-01, -2.9929e-01],
         [ 4.1886e-03,  8.3769e-03,  1.2565e-02,  ...,  1.2565e-01,
           1.2984e-01,  1.1340e+00]]], device='cuda:0',
       grad_fn=<LinalgMatrixExpBackward0>)


[NaN/Inf DETECTED] in A_inv_term at step 12
A_inv_term.min: 1.169488768937299e-06, max: inf
tensor([[[1.1695e-06, 2.3390e-06, 3.5086e-06,  ..., 3.5085e-05,
          3.6255e-05, 3.7423e-05],
         [2.3390e-06, 4.6781e-06, 7.0172e-06,  ..., 7.0170e-05,
          7.2509e-05, 7.4847e-05],
         [3.5085e-06, 7.0171e-06, 1.0526e-05,  ..., 1.0525e-04,
          1.0876e-04, 1.1227e-04],
         ...,
         [3.5085e-05, 7.0171e-05, 1.0526e-04,  ..., 1.0525e-03,
          1.0876e-03, 1.1227e-03],
         [3.6254e-05, 7.2510e-05, 1.0877e-04,  ..., 1.0876e-03,
          1.1239e-03, 1.1601e-03],
         [3.7424e-05, 7.4849e-05, 1.1227e-04,  ..., 1.1227e-03,
          1.1602e-03, 1.1975e-03]],

        [[1.1695e-06, 2.3390e-06, 3.5086e-06,  ..., 3.5085e-05,
          3.6255e-05, 3.7423e-05],
         [2.3390e-06, 4.6781e-06, 7.0172e-06,  ..., 7.0170e-05,
          7.2509e-05, 7.4847e-05],
         [3.5085e-06, 7.0171e-06, 1.0526e-05,  ..., 1.0525e-04,
          1.0876e-04, 1.1227e-04],
         ...,
         [3.5085e-05, 7.0171e-05, 1.0526e-04,  ..., 1.0525e-03,
          1.0876e-03, 1.1227e-03],
         [3.6254e-05, 7.2510e-05, 1.0877e-04,  ..., 1.0876e-03,
          1.1239e-03, 1.1601e-03],
         [3.7424e-05, 7.4849e-05, 1.1227e-04,  ..., 1.1227e-03,
          1.1602e-03, 1.1975e-03]],

        [[1.1695e-06, 2.3390e-06, 3.5086e-06,  ..., 3.5085e-05,
          3.6255e-05, 3.7423e-05],
         [2.3390e-06, 4.6781e-06, 7.0172e-06,  ..., 7.0170e-05,
          7.2509e-05, 7.4847e-05],
         [3.5085e-06, 7.0171e-06, 1.0526e-05,  ..., 1.0525e-04,
          1.0876e-04, 1.1227e-04],
         ...,
         [3.5085e-05, 7.0171e-05, 1.0526e-04,  ..., 1.0525e-03,
          1.0876e-03, 1.1227e-03],
         [3.6254e-05, 7.2510e-05, 1.0877e-04,  ..., 1.0876e-03,
          1.1239e-03, 1.1601e-03],
         [3.7424e-05, 7.4849e-05, 1.1227e-04,  ..., 1.1227e-03,
          1.1602e-03, 1.1975e-03]],

        ...,

        [[1.2146e-06, 2.4296e-06, 3.6433e-06,  ..., 3.6439e-05,
          3.7651e-05, 3.8867e-05],
         [2.4292e-06, 4.8592e-06, 7.2867e-06,  ..., 7.2878e-05,
          7.5303e-05, 7.7734e-05],
         [3.6439e-06, 7.2888e-06, 1.0930e-05,  ..., 1.0932e-04,
          1.1295e-04, 1.1660e-04],
         ...,
         [3.6439e-05, 7.2888e-05, 1.0930e-04,  ..., 1.0932e-03,
          1.1295e-03, 1.1660e-03],
         [3.7653e-05, 7.5318e-05, 1.1294e-04,  ..., 1.1296e-03,
          1.1672e-03, 1.2049e-03],
         [3.8868e-05, 7.7747e-05, 1.1659e-04,  ..., 1.1661e-03,
          1.2048e-03, 1.2437e-03]],

        [[1.2146e-06, 2.4296e-06, 3.6433e-06,  ..., 3.6439e-05,
          3.7651e-05, 3.8867e-05],
         [2.4292e-06, 4.8592e-06, 7.2867e-06,  ..., 7.2878e-05,
          7.5303e-05, 7.7734e-05],
         [3.6439e-06, 7.2888e-06, 1.0930e-05,  ..., 1.0932e-04,
          1.1295e-04, 1.1660e-04],
         ...,
         [3.6439e-05, 7.2888e-05, 1.0930e-04,  ..., 1.0932e-03,
          1.1295e-03, 1.1660e-03],
         [3.7653e-05, 7.5318e-05, 1.1294e-04,  ..., 1.1296e-03,
          1.1672e-03, 1.2049e-03],
         [3.8868e-05, 7.7747e-05, 1.1659e-04,  ..., 1.1661e-03,
          1.2048e-03, 1.2437e-03]],

        [[1.1695e-06, 2.3390e-06, 3.5086e-06,  ..., 3.5085e-05,
          3.6255e-05, 3.7423e-05],
         [2.3390e-06, 4.6781e-06, 7.0172e-06,  ..., 7.0170e-05,
          7.2509e-05, 7.4847e-05],
         [3.5085e-06, 7.0171e-06, 1.0526e-05,  ..., 1.0525e-04,
          1.0876e-04, 1.1227e-04],
         ...,
         [3.5085e-05, 7.0171e-05, 1.0526e-04,  ..., 1.0525e-03,
          1.0876e-03, 1.1227e-03],
         [3.6254e-05, 7.2510e-05, 1.0877e-04,  ..., 1.0876e-03,
          1.1239e-03, 1.1601e-03],
         [3.7424e-05, 7.4849e-05, 1.1227e-04,  ..., 1.1227e-03,
          1.1602e-03, 1.1975e-03]]], device='cuda:0', grad_fn=<BmmBackward0>)


[NaN/Inf DETECTED] in dB at step 12
dB.min: nan, max: nan
tensor([[[ 1.7310e-06, -1.1429e-05,  2.3070e-06,  ..., -1.5191e-05,
           1.5267e-05,  1.0306e-05],
         [ 3.4620e-06, -2.2859e-05,  4.6140e-06,  ..., -3.0381e-05,
           3.0535e-05,  2.0612e-05],
         [ 5.1930e-06, -3.4288e-05,  6.9210e-06,  ..., -4.5572e-05,
           4.5802e-05,  3.0918e-05],
         ...,
         [ 5.1930e-05, -3.4288e-04,  6.9210e-05,  ..., -4.5572e-04,
           4.5802e-04,  3.0918e-04],
         [ 5.3661e-05, -3.5431e-04,  7.1517e-05,  ..., -4.7091e-04,
           4.7329e-04,  3.1949e-04],
         [ 5.5392e-05, -3.6574e-04,  7.3824e-05,  ..., -4.8610e-04,
           4.8856e-04,  3.2979e-04]],

        [[ 1.7310e-06, -1.1429e-05,  2.3070e-06,  ..., -1.5191e-05,
           1.5267e-05,  1.0306e-05],
         [ 3.4620e-06, -2.2859e-05,  4.6140e-06,  ..., -3.0381e-05,
           3.0535e-05,  2.0612e-05],
         [ 5.1930e-06, -3.4288e-05,  6.9210e-06,  ..., -4.5572e-05,
           4.5802e-05,  3.0918e-05],
         ...,
         [ 5.1930e-05, -3.4288e-04,  6.9210e-05,  ..., -4.5572e-04,
           4.5802e-04,  3.0918e-04],
         [ 5.3661e-05, -3.5431e-04,  7.1517e-05,  ..., -4.7091e-04,
           4.7329e-04,  3.1949e-04],
         [ 5.5392e-05, -3.6574e-04,  7.3824e-05,  ..., -4.8610e-04,
           4.8856e-04,  3.2979e-04]],

        [[ 1.7310e-06, -1.1429e-05,  2.3070e-06,  ..., -1.5191e-05,
           1.5267e-05,  1.0306e-05],
         [ 3.4620e-06, -2.2859e-05,  4.6140e-06,  ..., -3.0381e-05,
           3.0535e-05,  2.0612e-05],
         [ 5.1930e-06, -3.4288e-05,  6.9210e-06,  ..., -4.5572e-05,
           4.5802e-05,  3.0918e-05],
         ...,
         [ 5.1930e-05, -3.4288e-04,  6.9210e-05,  ..., -4.5572e-04,
           4.5802e-04,  3.0918e-04],
         [ 5.3661e-05, -3.5431e-04,  7.1517e-05,  ..., -4.7091e-04,
           4.7329e-04,  3.1949e-04],
         [ 5.5392e-05, -3.6574e-04,  7.3824e-05,  ..., -4.8610e-04,
           4.8856e-04,  3.2979e-04]],

        ...,

        [[ 1.3779e-05, -1.1796e-06,  4.2824e-05,  ...,  2.2790e-05,
          -5.7721e-05, -1.5431e-04],
         [ 2.7557e-05, -2.3592e-06,  8.5648e-05,  ...,  4.5580e-05,
          -1.1544e-04, -3.0862e-04],
         [ 4.1336e-05, -3.5388e-06,  1.2847e-04,  ...,  6.8370e-05,
          -1.7316e-04, -4.6293e-04],
         ...,
         [ 4.1336e-04, -3.5388e-05,  1.2847e-03,  ...,  6.8370e-04,
          -1.7316e-03, -4.6293e-03],
         [ 4.2713e-04, -3.6568e-05,  1.3275e-03,  ...,  7.0649e-04,
          -1.7893e-03, -4.7836e-03],
         [ 4.4091e-04, -3.7747e-05,  1.3704e-03,  ...,  7.2928e-04,
          -1.8471e-03, -4.9379e-03]],

        [[ 1.3779e-05, -1.1796e-06,  4.2824e-05,  ...,  2.2790e-05,
          -5.7721e-05, -1.5431e-04],
         [ 2.7557e-05, -2.3592e-06,  8.5648e-05,  ...,  4.5580e-05,
          -1.1544e-04, -3.0862e-04],
         [ 4.1336e-05, -3.5388e-06,  1.2847e-04,  ...,  6.8370e-05,
          -1.7316e-04, -4.6293e-04],
         ...,
         [ 4.1336e-04, -3.5388e-05,  1.2847e-03,  ...,  6.8370e-04,
          -1.7316e-03, -4.6293e-03],
         [ 4.2713e-04, -3.6568e-05,  1.3275e-03,  ...,  7.0649e-04,
          -1.7893e-03, -4.7836e-03],
         [ 4.4091e-04, -3.7747e-05,  1.3704e-03,  ...,  7.2928e-04,
          -1.8471e-03, -4.9379e-03]],

        [[ 1.7310e-06, -1.1429e-05,  2.3070e-06,  ..., -1.5191e-05,
           1.5267e-05,  1.0306e-05],
         [ 3.4620e-06, -2.2859e-05,  4.6140e-06,  ..., -3.0381e-05,
           3.0535e-05,  2.0612e-05],
         [ 5.1930e-06, -3.4288e-05,  6.9210e-06,  ..., -4.5572e-05,
           4.5802e-05,  3.0918e-05],
         ...,
         [ 5.1930e-05, -3.4288e-04,  6.9210e-05,  ..., -4.5572e-04,
           4.5802e-04,  3.0918e-04],
         [ 5.3661e-05, -3.5431e-04,  7.1517e-05,  ..., -4.7091e-04,
           4.7329e-04,  3.1949e-04],
         [ 5.5392e-05, -3.6574e-04,  7.3824e-05,  ..., -4.8610e-04,
           4.8856e-04,  3.2979e-04]]], device='cuda:0', grad_fn=<BmmBackward0>)


NaN detectado en train_loss en epoch 0, batch 0. Deteniendo entrenamiento.


 real:
tensor([14, 14,  5,  2,  3,  4,  2,  2,  3, 14,  4,  3, 14, 14,  4,  4],
       device='cuda:0')


 pred:
tensor([14, 14,  3,  2,  3,  4,  2,  2,  3, 14,  4,  3, 14, 14,  4,  4],
       device='cuda:0')





 real:
tensor([14,  2,  4,  2,  2,  3,  5,  3, 14,  5,  2,  2,  3, 14,  3,  3],
       device='cuda:0')


 pred:
tensor([14,  2,  4,  2,  2,  4,  3,  3, 14,  3,  2,  2,  3, 14,  3,  3],
       device='cuda:0')





 real:
tensor([ 3,  3,  2,  4,  3,  4,  2,  4,  4,  3, 14,  4,  4,  4,  4,  1],
       device='cuda:0')


 pred:
tensor([ 3,  3,  2,  4,  3,  4,  2,  4,  4,  3, 14,  4,  4,  4,  4,  2],
       device='cuda:0')



Fold 1 Levenshtein Accuracy: 0.8292

=== Fold 2 ===
torch.Size([16])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2], device='cuda:0')



torch.Size([])
tensor(3, device='cuda:0')



Epoch 0: | Train Loss: 499455.844316 | Val Loss: 0.892248 | Train Acc: 44.532 | Val Acc: 80.438
Epoch 1: | Train Loss: 1.527844 | Val Loss: 0.774678 | Train Acc: 82.149 | Val Acc: 81.440
Epoch 2: | Train Loss: 0.663185 | Val Loss: 0.720364 | Train Acc: 83.238 | Val Acc: 83.681
Epoch 3: | Train Loss: 11.968389 | Val Loss: 0.729245 | Train Acc: 83.012 | Val Acc: 84.329
Epoch 4: | Train Loss: 2.250240 | Val Loss: 0.725431 | Train Acc: 83.414 | Val Acc: 84.234
Epoch 5: | Train Loss: 0.591219 | Val Loss: 0.736887 | Train Acc: 83.253 | Val Acc: 84.306
Epoch 6: | Train Loss: 0.541118 | Val Loss: 0.702324 | Train Acc: 83.282 | Val Acc: 84.306
Epoch 7: | Train Loss: 0.512113 | Val Loss: 0.697251 | Train Acc: 83.377 | Val Acc: 84.277
Epoch 8: | Train Loss: 0.520236 | Val Loss: 0.680273 | Train Acc: 83.443 | Val Acc: 84.352
Epoch 9: | Train Loss: 0.508862 | Val Loss: 0.720515 | Train Acc: 83.414 | Val Acc: 84.352
Epoch 10: | Train Loss: 1.449561 | Val Loss: 0.667612 | Train Acc: 83.399 | Val Acc: 84.352
Epoch 11: | Train Loss: 0.600612 | Val Loss: 0.742561 | Train Acc: 83.370 | Val Acc: 84.247
Epoch 12: | Train Loss: 0.575667 | Val Loss: 0.677147 | Train Acc: 83.289 | Val Acc: 84.352
Epoch 13: | Train Loss: 1.235359 | Val Loss: 0.767042 | Train Acc: 83.363 | Val Acc: 84.234
Epoch 14: | Train Loss: 1.648467 | Val Loss: 0.675912 | Train Acc: 83.297 | Val Acc: 84.352
Epoch 15: | Train Loss: 1.515649 | Val Loss: 0.715519 | Train Acc: 83.443 | Val Acc: 84.352
Epoch 16: | Train Loss: 0.505160 | Val Loss: 0.669810 | Train Acc: 83.465 | Val Acc: 84.352
Epoch 17: | Train Loss: 0.521174 | Val Loss: 0.703991 | Train Acc: 83.494 | Val Acc: 84.329
Epoch 18: | Train Loss: 0.686149 | Val Loss: 0.718289 | Train Acc: 83.319 | Val Acc: 84.211
Epoch 19: | Train Loss: 1.939616 | Val Loss: 0.696486 | Train Acc: 83.399 | Val Acc: 84.329
Epoch 20: | Train Loss: 0.997969 | Val Loss: 0.708527 | Train Acc: 83.070 | Val Acc: 84.352
Epoch 21: | Train Loss: 0.563582 | Val Loss: 8.972162 | Train Acc: 83.282 | Val Acc: 84.293
Epoch 22: | Train Loss: 0.566809 | Val Loss: 0.709064 | Train Acc: 83.392 | Val Acc: 84.329
Epoch 23: | Train Loss: 0.502146 | Val Loss: 0.667790 | Train Acc: 83.414 | Val Acc: 84.375
Epoch 24: | Train Loss: 0.503819 | Val Loss: 0.690495 | Train Acc: 83.414 | Val Acc: 84.329
Epoch 25: | Train Loss: 0.519971 | Val Loss: 0.677681 | Train Acc: 83.406 | Val Acc: 84.375
Epoch 26: | Train Loss: 2.124547 | Val Loss: 0.711081 | Train Acc: 83.209 | Val Acc: 84.329
Epoch 27: | Train Loss: 0.507231 | Val Loss: 0.667966 | Train Acc: 83.443 | Val Acc: 84.306
Epoch 28: | Train Loss: 0.502394 | Val Loss: 0.680930 | Train Acc: 83.341 | Val Acc: 84.352
Epoch 29: | Train Loss: 0.502078 | Val Loss: 0.699449 | Train Acc: 83.414 | Val Acc: 84.375
Epoch 30: | Train Loss: 0.499581 | Val Loss: 0.708034 | Train Acc: 83.428 | Val Acc: 84.034
Epoch 31: | Train Loss: 0.501055 | Val Loss: 0.699246 | Train Acc: 83.341 | Val Acc: 84.306
Epoch 32: | Train Loss: 0.499042 | Val Loss: 0.713850 | Train Acc: 83.465 | Val Acc: 83.828
Epoch 33: | Train Loss: 0.501866 | Val Loss: 0.699809 | Train Acc: 83.392 | Val Acc: 84.146
Epoch 34: | Train Loss: 0.497932 | Val Loss: 0.670320 | Train Acc: 83.531 | Val Acc: 84.306
Epoch 35: | Train Loss: 0.535459 | Val Loss: 0.715746 | Train Acc: 83.377 | Val Acc: 84.270
Epoch 36: | Train Loss: 0.496238 | Val Loss: 0.724284 | Train Acc: 83.545 | Val Acc: 84.087
Epoch 37: | Train Loss: 0.497511 | Val Loss: 0.692669 | Train Acc: 83.501 | Val Acc: 84.116
Epoch 38: | Train Loss: 0.496240 | Val Loss: 0.703331 | Train Acc: 83.531 | Val Acc: 82.590
Epoch 39: | Train Loss: 0.496887 | Val Loss: 0.695442 | Train Acc: 83.633 | Val Acc: 83.821
Epoch 40: | Train Loss: 0.505713 | Val Loss: 0.735784 | Train Acc: 83.545 | Val Acc: 82.288
Early stopping


 real:
tensor([ 3,  2,  2, 14,  2,  6,  4,  2,  3,  4,  3, 14,  3,  6,  3,  3],
       device='cuda:0')


 pred:
tensor([ 3,  2,  2, 14,  2,  3,  4,  2,  3,  4,  3, 14,  2,  3,  3,  2],
       device='cuda:0')





 real:
tensor([ 3,  2,  3,  3,  2,  2,  2,  1,  2, 14,  4,  3,  1,  4,  4,  4],
       device='cuda:0')


 pred:
tensor([ 3,  2,  3,  3,  2,  2,  2,  2,  2, 14,  4,  3,  2,  4,  4,  4],
       device='cuda:0')





 real:
tensor([ 2,  2, 14, 14,  3,  3,  2,  6,  2,  3, 14, 14,  4,  2,  3, 14],
       device='cuda:0')


 pred:
tensor([ 2,  2, 14, 14,  3,  3,  2,  3,  3,  3, 14, 14,  4,  2,  3, 14],
       device='cuda:0')



Fold 2 Levenshtein Accuracy: 0.8286

=== Fold 3 ===
torch.Size([16])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')



torch.Size([])
tensor(2, device='cuda:0')



Epoch 0: | Train Loss: 69752.279806 | Val Loss: 15866.385662 | Train Acc: 51.241 | Val Acc: 75.800
Epoch 1: | Train Loss: 31742.259048 | Val Loss: 14839.933242 | Train Acc: 54.947 | Val Acc: 59.173
Epoch 2: | Train Loss: 20095.519531 | Val Loss: 4538.796077 | Train Acc: 58.412 | Val Acc: 58.146
Epoch 3: | Train Loss: 15153.887496 | Val Loss: 5343.837136 | Train Acc: 55.619 | Val Acc: 67.930
NaN detectado en train_loss en epoch 4, batch 216. Deteniendo entrenamiento.


 real:
tensor([ 4, 14, 14,  4,  2,  4,  2,  5,  4,  4,  2,  7,  5,  5,  5,  5],
       device='cuda:0')


 pred:
tensor([14,  4,  4,  1,  2,  0, 14, 14,  4, 14,  2,  1, 14, 14, 14, 14],
       device='cuda:0')





 real:
tensor([ 5,  5,  2, 14,  4,  4,  4,  1,  2, 14,  5,  4, 14,  7, 14,  5],
       device='cuda:0')


 pred:
tensor([14, 14,  2,  6,  1,  0, 14,  2,  2,  6,  3, 14,  6,  6,  6, 14],
       device='cuda:0')





 real:
tensor([ 4,  4, 14,  5,  4,  3, 14, 14,  4, 14,  2,  2,  2,  5,  2, 14],
       device='cuda:0')


 pred:
tensor([14,  0,  4, 14,  0,  2,  4,  6,  0,  6,  2, 14,  2, 14,  2,  6],
       device='cuda:0')



Fold 3 Levenshtein Accuracy: 0.2345

=== Fold 4 ===
torch.Size([16])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2], device='cuda:0')



torch.Size([])
tensor(3, device='cuda:0')



Epoch 0: | Train Loss: 70756.916807 | Val Loss: 0.776273 | Train Acc: 75.095 | Val Acc: 83.581
Epoch 1: | Train Loss: 0.578759 | Val Loss: 0.667459 | Train Acc: 83.054 | Val Acc: 83.511
Epoch 2: | Train Loss: 0.699604 | Val Loss: 0.673269 | Train Acc: 83.049 | Val Acc: 83.557
Epoch 3: | Train Loss: 0.985394 | Val Loss: 0.761133 | Train Acc: 82.944 | Val Acc: 83.458
Epoch 4: | Train Loss: 6.091423 | Val Loss: 0.707575 | Train Acc: 82.978 | Val Acc: 83.575
Epoch 5: | Train Loss: 4.160154 | Val Loss: 0.701506 | Train Acc: 83.025 | Val Acc: 83.353
Epoch 6: | Train Loss: 53.037404 | Val Loss: 0.677614 | Train Acc: 83.010 | Val Acc: 83.627
Epoch 7: | Train Loss: 0.533510 | Val Loss: 0.704359 | Train Acc: 83.328 | Val Acc: 83.592
Epoch 8: | Train Loss: 0.512566 | Val Loss: 0.713570 | Train Acc: 83.243 | Val Acc: 83.627
Epoch 9: | Train Loss: 0.509980 | Val Loss: 0.726440 | Train Acc: 83.192 | Val Acc: 83.610
Epoch 10: | Train Loss: 0.509721 | Val Loss: 0.742572 | Train Acc: 83.243 | Val Acc: 83.534
Epoch 11: | Train Loss: 1.461213 | Val Loss: 0.717214 | Train Acc: 83.180 | Val Acc: 83.610
Epoch 12: | Train Loss: 0.972681 | Val Loss: 1.275830 | Train Acc: 83.122 | Val Acc: 83.364
Epoch 13: | Train Loss: 0.551070 | Val Loss: 0.700750 | Train Acc: 83.309 | Val Acc: 83.534
Epoch 14: | Train Loss: 0.536129 | Val Loss: 0.701590 | Train Acc: 83.355 | Val Acc: 83.569
Epoch 15: | Train Loss: 0.605368 | Val Loss: 0.722177 | Train Acc: 83.353 | Val Acc: 83.353
Epoch 16: | Train Loss: 0.504737 | Val Loss: 0.708035 | Train Acc: 83.348 | Val Acc: 83.318
Epoch 17: | Train Loss: 0.508703 | Val Loss: 0.704416 | Train Acc: 83.294 | Val Acc: 83.353
Epoch 18: | Train Loss: 0.532515 | Val Loss: 0.674721 | Train Acc: 83.606 | Val Acc: 83.534
Epoch 19: | Train Loss: 0.536950 | Val Loss: 0.686834 | Train Acc: 83.491 | Val Acc: 83.318
Epoch 20: | Train Loss: 21.583297 | Val Loss: 0.703797 | Train Acc: 83.367 | Val Acc: 83.505
Epoch 21: | Train Loss: 0.522264 | Val Loss: 0.684119 | Train Acc: 83.535 | Val Acc: 83.540
Epoch 22: | Train Loss: 97.111736 | Val Loss: 0.736613 | Train Acc: 83.620 | Val Acc: 83.487
Epoch 23: | Train Loss: 6.997252 | Val Loss: 0.714574 | Train Acc: 83.438 | Val Acc: 83.627
Epoch 24: | Train Loss: 7.714996 | Val Loss: 0.721714 | Train Acc: 82.995 | Val Acc: 83.551
Epoch 25: | Train Loss: 1.002455 | Val Loss: 0.722986 | Train Acc: 83.112 | Val Acc: 83.522
Epoch 26: | Train Loss: 0.521488 | Val Loss: 0.713941 | Train Acc: 83.511 | Val Acc: 83.452
Epoch 27: | Train Loss: 0.654205 | Val Loss: 0.687744 | Train Acc: 83.348 | Val Acc: 83.569
Epoch 28: | Train Loss: 110.681416 | Val Loss: 0.725655 | Train Acc: 83.474 | Val Acc: 83.522
Epoch 29: | Train Loss: 0.648462 | Val Loss: 0.714068 | Train Acc: 83.309 | Val Acc: 83.248
Epoch 30: | Train Loss: 1.809687 | Val Loss: 0.741248 | Train Acc: 83.406 | Val Acc: 83.598
Epoch 31: | Train Loss: 0.662035 | Val Loss: 0.730004 | Train Acc: 83.197 | Val Acc: 83.598
Early stopping


 real:
tensor([4, 2, 2, 3, 1, 3, 2, 4, 4, 1, 3, 4, 4, 3, 4, 4], device='cuda:0')


 pred:
tensor([4, 2, 2, 3, 2, 3, 3, 4, 4, 2, 3, 4, 4, 2, 4, 4], device='cuda:0')





 real:
tensor([ 3,  2,  4,  4,  2,  2,  5,  4, 14,  3,  3,  2,  4, 14,  3,  3],
       device='cuda:0')


 pred:
tensor([ 3,  2,  4,  4,  2,  2,  3,  4, 14,  3,  3,  2,  4, 14,  1,  3],
       device='cuda:0')





 real:
tensor([ 3,  3,  7, 14,  3,  2,  3,  4, 14,  3,  3, 14,  2, 14, 14,  5],
       device='cuda:0')


 pred:
tensor([ 3,  3,  3, 14,  2,  2,  2,  4, 14,  3,  3, 14,  2, 14, 14,  3],
       device='cuda:0')



Fold 4 Levenshtein Accuracy: 0.8363
Average Levenshtein Accuracy: 0.7114
